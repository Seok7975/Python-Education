{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ac32fe-7ea6-4275-91e3-1be0db15fe5f",
   "metadata": {},
   "source": [
    "## 언어모델 제어하기\n",
    "\n",
    "#### top_p\n",
    "\n",
    "temperature가 확률분포를 조정하는 매개변수라면, top_p는 확률분포 내에서 선택할 단어의 범위를 결정하는 매개변수입니다. top_p는 확률 역순으로 단어를 정렬한 후, 그 순서대로 단어를 선택해 가다가 누적 확률이 top_p에 도달하는 순간 선택을 멈추는 방식으로 동작합니다. 예를 들어 temperature=0.25, top_p=0.6으로 설정했다면 다음 그래프처럼 “오른다”와 “간다”를 누적하는 순간 0.6에 도달합니다. 따라서 “왔다” 이후의 단어들은 선택에서 제외됩니다.\n",
    "\n",
    "<center>\n",
    "<img src='https://wikidocs.net/images/page/229816/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-02-04_093948.png' /><br>\n",
    "</center>\n",
    "\n",
    "이렇게 선택된 두 개의 단어는 다음의 확률분포로 다시 만들어집니다.\n",
    "\n",
    "- “오른다” : 0.46 / (0.46 + 0.25) = 0.648\n",
    "\n",
    "- “간다” : 0.25 / (0.46 + 0.25) = 0.352\n",
    "\n",
    "그리고 이 두 개의 확률분포를 바탕으로 언어모델은 최종 문장을 만듭니다. 다음은 이러한 상황을 가정하고 100회 실시한 결과입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809abd70-1291-40d6-8d83-7789835396ca",
   "metadata": {},
   "source": [
    "probs = [0.46, 0.25]\n",
    "probs = list(map(lambda x : round(x / sum(probs),3), probs)) #[0.648, 0.352]\n",
    "context = \"나는 내일 산에 \"\n",
    "word_occurrences = count_word_occurrences(context, [\"오른다\",\"간다\"], probs, 100)\n",
    "word_occurrences =  {k: v for k, v in sorted(word_occurrences.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dff136-2f31-4283-b108-1bade914e2bf",
   "metadata": {},
   "source": [
    "word_occurrences <br/>\n",
    "{'나는 내일 산에 오른다': 67, '나는 내일 산에 간다': 33}\n",
    "\n",
    "만일, top_p=0으로 설정한다면 확률분포 중 가장 높은 확률의 단어만 선택하게 되므로 temperature와 관계 없이 항상 일관된 답변을 기대할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077a3d6a-3d85-4916-9e74-90ca01145a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top_p=0:\n",
      "==================================================\n",
      "눈 내리고, 추위 몰아치고,\n",
      "겨울의 품에 안겨.\n",
      "==================================================\n",
      "눈 내리고, 추위 몰아치고,\n",
      "겨울의 품에 안겨.\n",
      "==================================================\n",
      "눈 내리고, 추위 몰아치고,\n",
      "겨울의 품에 안겨.\n",
      "\n",
      "top_p=1:\n",
      "==================================================\n",
      "눈꽃 흩날리네\n",
      "땅이 흰 옷 입었네\n",
      "겨울이 왔네\n",
      "==================================================\n",
      "흰 눈 내리는,\n",
      "나뭇가지 우거진 겨울,\n",
      "고요한 침묵.\n",
      "==================================================\n",
      "백설이 쌓인 땅\n",
      "추위가 물든 바람\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyD0jMkUWWxa6O1qpGjMIz80zOVxM4KoyKU\")\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "user_message = \"겨울에 대한 짧은 시를 20자 이내로 지으세요.\"\n",
    "\n",
    "print(\"\\ntop_p=0:\")\n",
    "generation_config = genai.GenerationConfig(top_p=0)\n",
    "for _ in range(3):\n",
    "    response = model.generate_content(user_message , generation_config=generation_config)\n",
    "    print(f'{\"=\"*50}\\n{response.text}')\n",
    "\n",
    "print(\"\\ntop_p=1:\")\n",
    "generation_config = genai.GenerationConfig(top_p=1)\n",
    "for _ in range(3):\n",
    "    response = model.generate_content(user_message , generation_config=generation_config)\n",
    "    print(f'{\"=\"*50}\\n{response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ab34f-0ab4-4331-9341-621491d07e30",
   "metadata": {},
   "source": [
    "top_p=0으로 설정하고 수행했을 때 기대했던 대로 3번 모두 동일한 결과가 출력되었습니다. 거기에 더해 temperature=0 일때와 동일한 내용의 시를 쓴 것을 확인할 수 있습니다. 이에 비해 top_p=1로 설정하고 수행했을 때는, temperature=0으로 설정했을 때와 마찬가지로, 매번 다른 내용의 시를 쓴 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db63c1-f002-40c1-9dcc-7587eb001b9c",
   "metadata": {},
   "source": [
    "알아두기=============================================<br/>\n",
    "temperature=0이나 top_p=0으로 설정했다고 해서 매번 완전히 동일한 결과만 생성하는 것은 아닙니다. 거대언어모델은 클라우드 환경에서 여러 노드의 리소스를 병렬적으로 사용하는 방식으로 수많은 연산을 수행할 뿐만 아니라, 확률분포에 동일한 확률값을 갖는 단어들도 나타날 수 있으므로 항상 같은 결과를 보장하기는 어렵습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93b7bd-296d-4e24-a951-0d1156d21737",
   "metadata": {},
   "source": [
    "#### top_k\n",
    "\n",
    "top_p를 이해했다면, top_k를 이해하기는 매우 쉽습니다. top_p가 누적 확률을 기준으로 선택할 단어의 범위를 결정한다면, top_k는 그 기준이 누적 건수라는 점만 다르기 때문입니다. 가령 다음과 같이 temperature=0.25, top_k=2로 설정했다면 “오른다”와 “간다” 두 개의 단어만 선택되며, 그 이후의 동작은 top_p와 동일합니다.\n",
    "\n",
    "<center>\n",
    "<img src='https://wikidocs.net/images/page/229817/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-02-04_094622.png' /><br>\n",
    "</center>\n",
    "\n",
    "top_k는 top_p에 비해 매개변수 조정이 권장되지 않는 측면이 있습니다. 설명에서 알 수 있듯이 k개의 단어가 선택되는 과정에서 단어 간의 확률 편차가 고려되지 않기 때문입니다. 이에 비해 top_p는 확률 분포의 ‘긴 꼬리’를 자르기 때문에 보다 자연스러운 텍스트 생성을 가능하게 합니다. 구글 제미나이 API에서는 top_k의 초깃값을 64로 두고 있으며, 특별한 이유가 없다면 이 값을 그대로 사용하기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6011412-6083-45f5-895b-bed2aaf5933d",
   "metadata": {},
   "source": [
    "#### 매개변수 요약표\n",
    "\n",
    "지금까지 학습한 구글 제미나이 AI의 매개변수를 요약하면 다음과 같습니다.\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>매개변수명</th>\n",
    "<th>의미</th>\n",
    "<th style=\"text-align: center;\">초깃값</th>\n",
    "<th style=\"text-align: center;\">범위</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>candidate_count</td>\n",
    "<td>생성할 응답 후보 건수. 현재는 1만 가능</td>\n",
    "<td style=\"text-align: center;\">1</td>\n",
    "<td style=\"text-align: center;\">1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>stop_sequences</td>\n",
    "<td>언어 생성을 중지시킬 문자 시퀀스</td>\n",
    "<td style=\"text-align: center;\">없음</td>\n",
    "<td style=\"text-align: center;\">0 ~ 5</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>max_output_tokens</td>\n",
    "<td>출력할 최대 토큰 수</td>\n",
    "<td style=\"text-align: center;\">8192</td>\n",
    "<td style=\"text-align: center;\">1 ~ 8192</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>temperature</td>\n",
    "<td>출력의 무작위성을 제어</td>\n",
    "<td style=\"text-align: center;\">1.0</td>\n",
    "<td style=\"text-align: center;\">0.0 ~ 2.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>top_p</td>\n",
    "<td>확률 내림차순으로 정렬 후 누적 확률 기준으로 <br>선택할 단어(토큰)의 범위를 설정</td>\n",
    "<td style=\"text-align: center;\">0.95</td>\n",
    "<td style=\"text-align: center;\">0.0 ~ 1.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>top_k</td>\n",
    "<td>확률 내림차순으로 정렬 후 건수 기준으로<br> 선택할 단어(토큰)의 범위를 설정</td>\n",
    "<td style=\"text-align: center;\">64</td>\n",
    "<td style=\"text-align: center;\">0보다 큰 정수</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c3f7e-8114-44c2-bb26-2f56ed426468",
   "metadata": {},
   "source": [
    "한편, 매개변수의 초깃값은 다음 명령으로도 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd800349-1ff3-4731-85c8-c4658c9ae90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "print(genai.get_model(\"models/gemini-1.5-flash\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
